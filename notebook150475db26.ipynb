{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":10975004,"sourceType":"datasetVersion","datasetId":6829250}],"dockerImageVersionId":30919,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:26:30.519203Z","iopub.execute_input":"2025-03-10T01:26:30.519634Z","iopub.status.idle":"2025-03-10T01:26:30.914334Z","shell.execute_reply.started":"2025-03-10T01:26:30.519591Z","shell.execute_reply":"2025-03-10T01:26:30.912390Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install transformers peft accelerate bitsandbytes datasets trl -q --upgrade","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T01:51:03.980267Z","iopub.execute_input":"2025-03-10T01:51:03.980594Z","iopub.status.idle":"2025-03-10T01:51:08.296435Z","shell.execute_reply.started":"2025-03-10T01:51:03.980567Z","shell.execute_reply":"2025-03-10T01:51:08.294729Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m318.9/318.9 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from datasets import load_dataset\n\n# Load the CSV file (adjust the file path as needed)\ndataset = load_dataset(\"csv\", data_files={\"train\": \"/kaggle/input/synthetic-qa-dataset-based-on-ai-research-papers/combined_alpaca_dataset.csv\"})[\"train\"]\n\n# Check existing columns (expecting at least 'instruction' and 'output')\nprint(dataset.column_names)\n\n# If the dataset lacks an \"input\" column, add one with empty strings:\ndef add_empty_input(example):\n    if \"input\" not in example:\n        example[\"input\"] = \"\"\n    return example\n\ndataset = dataset.map(add_empty_input)\nprint(\"Columns after update:\", dataset.column_names)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:12:47.976395Z","iopub.execute_input":"2025-03-10T02:12:47.976743Z","iopub.status.idle":"2025-03-10T02:12:48.309974Z","shell.execute_reply.started":"2025-03-10T02:12:47.976716Z","shell.execute_reply":"2025-03-10T02:12:48.308974Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Generating train split: 0 examples [00:00, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f1e073ba74914efaacd9ead7622181de"}},"metadata":{}},{"name":"stdout","text":"['instruction', 'output', 'source', 'metadata']\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/264 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0e9f70372d1a4577b4917987f92c0d88"}},"metadata":{}},{"name":"stdout","text":"Columns after update: ['instruction', 'output', 'source', 'metadata', 'input']\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"split_dataset = dataset.train_test_split(test_size=0.1, seed=49)\ntrain_dataset = split_dataset[\"train\"]\nval_dataset = split_dataset[\"test\"]\n\nprint(f\"Training set size: {len(train_dataset)}\")\nprint(f\"Validation set size: {len(val_dataset)}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:13:38.093015Z","iopub.execute_input":"2025-03-10T02:13:38.093301Z","iopub.status.idle":"2025-03-10T02:13:38.108078Z","shell.execute_reply.started":"2025-03-10T02:13:38.093280Z","shell.execute_reply":"2025-03-10T02:13:38.107152Z"}},"outputs":[{"name":"stdout","text":"Training set size: 237\nValidation set size: 27\n","output_type":"stream"}],"execution_count":13},{"cell_type":"code","source":"train_dataset[7]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:13:40.575866Z","iopub.execute_input":"2025-03-10T02:13:40.576187Z","iopub.status.idle":"2025-03-10T02:13:40.581968Z","shell.execute_reply.started":"2025-03-10T02:13:40.576161Z","shell.execute_reply":"2025-03-10T02:13:40.581069Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"{'instruction': 'Explain how DeepSeek\\'s DualPipe addresses the \"bubble\" problem in distributed GPU training, and analyze the trade-offs between communication reduction and potential synchronization overhead introduced by processing batches in opposite directions.',\n 'output': 'DeepSeek\\'s DualPipe addresses the \"bubble\" problem, a training inefficiency where GPUs remain idle waiting for data, by overlapping forward and backward passes. In conventional distributed training, GPUs process data sequentially, leading to idle time (the \"bubble\") while waiting for data from preceding GPUs.\\n\\nDualPipe innovatively initiates training data from two devices in opposite directions. While device 0 processes batch 0 in the forward pass, device 7 (in a system with 8 GPUs) starts processing a different batch in the reverse direction. This concurrent processing reduces idle time. Instead of device 7 waiting for batch 0, it immediately begins computation. The forward and backward passes are combined as a \"chunk\" and continuously copied together, reducing communication volume between GPUs.\\n\\nThe trade-off involves potential synchronization overhead. Processing batches in opposite directions might require careful synchronization to ensure data consistency and avoid conflicts. While DualPipe reduces communication volume, the synchronization mechanism itself could introduce latency. The success of DualPipe hinges on minimizing this synchronization overhead to ensure that the reduction in communication outweighs any added latency. The document doesn\\'t explicitly quantify this trade-off, but the fact that DeepSeek implemented it suggests that the communication reduction benefits outweighed the synchronization costs in their experiments with H800 GPUs.',\n 'source': './data/deepseekv3-explained.md',\n 'metadata': '{}',\n 'input': ''}"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM, AutoTokenizer\n\n# Replace with your chosen instruct model id; for example, using Qwen2.5-3B-Instruct\nmodel_id = \"Qwen/Qwen2.5-3B-Instruct\"\n\ntokenizer = AutoTokenizer.from_pretrained(model_id)\n# Ensure the pad token is set (often to the EOS token)\ntokenizer.pad_token = tokenizer.eos_token\n\n# Load the model (without quantization in this case; plain LoRA is sufficient)\nmodel = AutoModelForCausalLM.from_pretrained(model_id, torch_dtype=torch.float16)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:14:05.446101Z","iopub.execute_input":"2025-03-10T02:14:05.446528Z","iopub.status.idle":"2025-03-10T02:14:43.976767Z","shell.execute_reply.started":"2025-03-10T02:14:05.446485Z","shell.execute_reply":"2025-03-10T02:14:43.975809Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/7.30k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddf4351460a04223898cda7ba91e4563"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4e987ce290cd4b1a98ae5e27499c01a5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"69789e0b28024badbe596bb5403ad849"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"754b76c7204f48958bf8e4ec21293212"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/661 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"068967bc20914838811f575ea63787e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json:   0%|          | 0.00/35.6k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3898496490fc4761801600f8e934a554"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d0ae082d14a7422a87d7f7db04c808a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/3.97G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0d7646b92c9400ba4d5f7925eb514fc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/2.20G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"966c2caace2e413995610199bdbf6ee3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9d87c70023245fa96979cb04bc200ba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10cd4b2ee83942088c91730d4e3bb4c8"}},"metadata":{}}],"execution_count":15},{"cell_type":"code","source":"from peft import prepare_model_for_kbit_training, LoraConfig, get_peft_model\n\n# Enable gradient checkpointing (optional but useful for memory saving)\nmodel.gradient_checkpointing_enable()\n\n# For plain LoRA, we don’t need to quantize the base model further\n# Prepare the model for adapter training (freezes most base parameters)\n# (prepare_model_for_kbit_training is typically used for QLoRA; for plain LoRA, you may skip it,\n# but using it doesn't hurt if your base model is large)\nmodel = prepare_model_for_kbit_training(model)\n\n# Set up LoRA configuration:\npeft_config = LoraConfig(\n    task_type=\"CAUSAL_LM\",\n    r=16,                  # Low-rank dimension (try 16 or 32; lower values are safer for small datasets)\n    lora_alpha=32,         # Scaling factor for adapter updates\n    lora_dropout=0.1,      # Dropout for adapter layers to reduce overfitting\n    target_modules=[\"q_proj\", \"v_proj\"],  # Typical target modules in the Transformer\n    bias=\"none\"\n)\n\n# Apply LoRA to the model:\nlora_model = get_peft_model(model, peft_config)\nlora_model.print_trainable_parameters()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:14:51.879775Z","iopub.execute_input":"2025-03-10T02:14:51.880117Z","iopub.status.idle":"2025-03-10T02:14:58.575231Z","shell.execute_reply.started":"2025-03-10T02:14:51.880090Z","shell.execute_reply":"2025-03-10T02:14:58.574361Z"}},"outputs":[{"name":"stdout","text":"trainable params: 3,686,400 || all params: 3,089,625,088 || trainable%: 0.1193\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"def tokenize_function(batch):\n    prompts = []\n    for instruction, inp, output in zip(batch[\"instruction\"], batch[\"input\"], batch[\"output\"]):\n        # Check if the input is empty after stripping whitespace.\n        if inp.strip() == \"\":\n            prompt = (\n                \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n                f\"### Instruction:\\n{instruction}\\n\\n\"\n                f\"### Response:\\n{output}\"\n            )\n        else:\n            prompt = (\n                \"Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\\n\\n\"\n                f\"### Instruction:\\n{instruction}\\n\\n\"\n                f\"### Input:\\n{inp}\\n\\n\"\n                f\"### Response:\\n{output}\"\n            )\n        prompts.append(prompt)\n    # Tokenize all prompts together\n    return tokenizer(prompts, truncation=True, max_length=512, padding=\"max_length\")\n    \ntokenized_train = train_dataset.map(tokenize_function, batched=True)\ntokenized_val = val_dataset.map(tokenize_function, batched=True)\n\n# Optionally inspect a sample tokenized output:\nprint(tokenized_train[7])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:23:14.645864Z","iopub.execute_input":"2025-03-10T02:23:14.646273Z","iopub.status.idle":"2025-03-10T02:23:15.050955Z","shell.execute_reply.started":"2025-03-10T02:23:14.646226Z","shell.execute_reply":"2025-03-10T02:23:15.050125Z"},"collapsed":true,"jupyter":{"outputs_hidden":true}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/237 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b1ec182d5172443ab3be22dce51c06bc"}},"metadata":{}},{"name":"stdout","text":"{'instruction': 'Explain how DeepSeek\\'s DualPipe addresses the \"bubble\" problem in distributed GPU training, and analyze the trade-offs between communication reduction and potential synchronization overhead introduced by processing batches in opposite directions.', 'output': 'DeepSeek\\'s DualPipe addresses the \"bubble\" problem, a training inefficiency where GPUs remain idle waiting for data, by overlapping forward and backward passes. In conventional distributed training, GPUs process data sequentially, leading to idle time (the \"bubble\") while waiting for data from preceding GPUs.\\n\\nDualPipe innovatively initiates training data from two devices in opposite directions. While device 0 processes batch 0 in the forward pass, device 7 (in a system with 8 GPUs) starts processing a different batch in the reverse direction. This concurrent processing reduces idle time. Instead of device 7 waiting for batch 0, it immediately begins computation. The forward and backward passes are combined as a \"chunk\" and continuously copied together, reducing communication volume between GPUs.\\n\\nThe trade-off involves potential synchronization overhead. Processing batches in opposite directions might require careful synchronization to ensure data consistency and avoid conflicts. While DualPipe reduces communication volume, the synchronization mechanism itself could introduce latency. The success of DualPipe hinges on minimizing this synchronization overhead to ensure that the reduction in communication outweighs any added latency. The document doesn\\'t explicitly quantify this trade-off, but the fact that DeepSeek implemented it suggests that the communication reduction benefits outweighed the synchronization costs in their experiments with H800 GPUs.', 'source': './data/deepseekv3-explained.md', 'metadata': '{}', 'input': '', 'input_ids': [38214, 374, 458, 7600, 429, 16555, 264, 3383, 13, 9645, 264, 2033, 429, 34901, 44595, 279, 1681, 382, 14374, 29051, 510, 840, 20772, 1246, 18183, 39350, 594, 33659, 34077, 14230, 279, 330, 77489, 1, 3491, 304, 4237, 22670, 4862, 11, 323, 23643, 279, 6559, 63939, 1948, 10535, 13951, 323, 4650, 57912, 31015, 11523, 553, 8692, 44792, 304, 14002, 17961, 382, 14374, 5949, 510, 33464, 39350, 594, 33659, 34077, 14230, 279, 330, 77489, 1, 3491, 11, 264, 4862, 89225, 10387, 1380, 70403, 7146, 27647, 8580, 369, 821, 11, 553, 49817, 4637, 323, 27555, 16211, 13, 758, 20692, 4237, 4862, 11, 70403, 1882, 821, 94559, 11, 6388, 311, 27647, 882, 320, 1782, 330, 77489, 899, 1393, 8580, 369, 821, 504, 37746, 70403, 382, 85074, 34077, 9529, 7887, 12672, 973, 4862, 821, 504, 1378, 7611, 304, 14002, 17961, 13, 5976, 3671, 220, 15, 11364, 7162, 220, 15, 304, 279, 4637, 1494, 11, 3671, 220, 22, 320, 258, 264, 1849, 448, 220, 23, 70403, 8, 8471, 8692, 264, 2155, 7162, 304, 279, 9931, 5106, 13, 1096, 34035, 8692, 25271, 27647, 882, 13, 12090, 315, 3671, 220, 22, 8580, 369, 7162, 220, 15, 11, 432, 7069, 12033, 34447, 13, 576, 4637, 323, 27555, 16211, 525, 10856, 438, 264, 330, 25979, 1, 323, 30878, 21774, 3786, 11, 17719, 10535, 8123, 1948, 70403, 382, 785, 6559, 12462, 17601, 4650, 57912, 31015, 13, 28125, 44792, 304, 14002, 17961, 2578, 1373, 16585, 57912, 311, 5978, 821, 28137, 323, 5648, 25800, 13, 5976, 33659, 34077, 25271, 10535, 8123, 11, 279, 57912, 16953, 5086, 1410, 19131, 39270, 13, 576, 2393, 315, 33659, 34077, 79834, 389, 76291, 419, 57912, 31015, 311, 5978, 429, 279, 13951, 304, 10535, 61974, 82, 894, 3694, 39270, 13, 576, 2197, 3171, 944, 20975, 75398, 419, 6559, 12462, 11, 714, 279, 2097, 429, 18183, 39350, 11537, 432, 13230, 429, 279, 10535, 13951, 7567, 61974, 291, 279, 57912, 7049, 304, 862, 21204, 448, 472, 23, 15, 15, 70403, 13, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645, 151645], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n","output_type":"stream"}],"execution_count":19},{"cell_type":"code","source":"from transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\n# Updated training arguments (note: you can switch to eval_strategy in newer versions)\ntraining_args = TrainingArguments(\n    output_dir=\"./output\",                   # Directory for saving model checkpoints.\n    per_device_train_batch_size=2,           # Small batch size to manage memory.\n    per_device_eval_batch_size=2,            # Small eval batch size.\n    gradient_accumulation_steps=4,           # Simulate a larger effective batch size.\n    num_train_epochs=5,                      # More epochs might be needed for a small dataset.\n    learning_rate=2e-4,                      # Starting learning rate.\n    weight_decay=0.0,                        # No weight decay; adjust if overfitting occurs.\n    warmup_steps=20,                         # Short warmup period.\n    logging_steps=10,                        # Log training metrics every 10 steps.\n    evaluation_strategy=\"steps\",             # Evaluate every eval_steps.\n    eval_steps=20,                           # Evaluation frequency.\n    save_strategy=\"steps\",                   # Save checkpoint every few steps.\n    save_steps=20,                           # Checkpoint frequency.\n    save_total_limit=2,                      # Keep only a couple of checkpoints.\n    fp16=True,                               # Enable mixed precision for faster training.\n    load_best_model_at_end=True,             # Load best model based on eval loss.\n    run_name=\"my_unique_run_name\",           # Distinct run name to avoid wandb warnings.\n)\n\n# Data collator for dynamic padding:\ndata_collator = DataCollatorForSeq2Seq(tokenizer, model=lora_model, padding=True)\n\ntrainer = Trainer(\n    model=lora_model,\n    args=training_args,\n    train_dataset=tokenized_train,\n    eval_dataset=tokenized_val,\n    data_collator=data_collator,\n)\n\n# Disable cache to save memory during training.\nlora_model.config.use_cache = False\n\n# Start training:\ntrainer.train()\n\n# Save the fine-tuned model and tokenizer:\nlora_model.save_pretrained(\"output/finetuned_model\")\ntokenizer.save_pretrained(\"output/finetuned_model\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-10T02:26:43.654137Z","iopub.execute_input":"2025-03-10T02:26:43.654523Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.Javascript object>","application/javascript":"\n        window._wandbApiKey = new Promise((resolve, reject) => {\n            function loadScript(url) {\n            return new Promise(function(resolve, reject) {\n                let newScript = document.createElement(\"script\");\n                newScript.onerror = reject;\n                newScript.onload = resolve;\n                document.body.appendChild(newScript);\n                newScript.src = url;\n            });\n            }\n            loadScript(\"https://cdn.jsdelivr.net/npm/postmate/build/postmate.min.js\").then(() => {\n            const iframe = document.createElement('iframe')\n            iframe.style.cssText = \"width:0;height:0;border:none\"\n            document.body.appendChild(iframe)\n            const handshake = new Postmate({\n                container: iframe,\n                url: 'https://wandb.ai/authorize'\n            });\n            const timeout = setTimeout(() => reject(\"Couldn't auto authenticate\"), 5000)\n            handshake.then(function(child) {\n                child.on('authorize', data => {\n                    clearTimeout(timeout)\n                    resolve(data)\n                });\n            });\n            })\n        });\n    "},"metadata":{}}],"execution_count":null},{"cell_type":"code","source":"# Example prompt for inference:\ntest_prompt = (\n    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n    \"### Instruction:\\nWhat is the meaning of life?\\n\\n\"\n    \"### Response:\\n\"\n)\n\n# Tokenize the test prompt:\ninputs = tokenizer(test_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\n# Generate response (adjust max_new_tokens as needed):\nwith torch.no_grad():\n    outputs = lora_model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Merge the LoRA weights into the base model.\n# (This function is provided by the PEFT library.)\nmerged_model = lora_model.merge_and_unload()\n\n# Save the merged model for later use.\nmerged_model.save_pretrained(\"output/qwen_ai_research_qa_final\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import BitsAndBytesConfig\n\nquant_config = BitsAndBytesConfig(\n    load_in_4bit=True,                # Enable 4-bit quantization.\n    bnb_4bit_quant_type=\"nf4\",        # Use the NormalFloat 4 (NF4) quantization type.\n    bnb_4bit_use_double_quant=True,   # Activate double quantization for additional precision.\n    bnb_4bit_compute_dtype=torch.bfloat16  # Use bfloat16 for computations during forward/backward passes.\n)\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from transformers import AutoModelForCausalLM\n\nquantized_model = AutoModelForCausalLM.from_pretrained(\n    \"output/merged_model\", \n    quantization_config=quant_config, \n    torch_dtype=torch.bfloat16  # Ensure compatibility with your compute dtype.\n)\n\n# Save the quantized model for future inference.\nquantized_model.save_pretrained(\"output/qwen_ai_research_qa_final_Q4\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Prepare an inference prompt.\ntest_prompt = (\n    \"Below is an instruction that describes a task. Write a response that appropriately completes the request.\\n\\n\"\n    \"### Instruction:\\nWhat is the meaning of life?\\n\\n\"\n    \"### Response:\\n\"\n)\n\ninputs = tokenizer(test_prompt, return_tensors=\"pt\", padding=True, truncation=True).to(\"cuda\")\n\nwith torch.no_grad():\n    outputs = quantized_model.generate(inputs[\"input_ids\"], max_new_tokens=50)\n\nprint(tokenizer.decode(outputs[0], skip_special_tokens=True))\n","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}